in this chapter we learn about caching caching is a fundamental concept of computer science in fact any system that you pick up any large scale distributed system has some form of caching in multiple places and often in critical sections so in this chapter we will dive into what caching is what are the benefits what are the potential drawbacks and how we can mitigate those drawbacks using tradeoffs let's take an example you have a user on Instagram who is asking for their feed their news feed so the message reaches the server that please get me my newsfeed the server queries the database says that for this user select star where user equal to so and so get me all the people that they're following and get me the posts of those people the response comes back and is forwarded to the client the overall time required for this is user to server which in our case is 100 milliseconds server to database which is 10 milliseconds database back to server this is the response taking 10 milliseconds and the server finally sending back the response to the client which is again 100 milliseconds so the total time here will be 220 Mills if you had to optimize this system what would you do there would be two parts that you can look at the first is client to server communication and the second is server to database communication when it comes to caching on the back end we usually focus on the second part which is server to database in our case you can see that it doesn't have as big impact as client to server so that is also important we'll look at that maybe in some other chapter here we are focusing on the backend engineering side so there may be many many services connecting to the database if you can introduce some optimization here you will cut down this 10 milliseconds to maybe 1 millisecond now one optimization here is that similar users ask for similar feeds so for example a young software engineer who likes football and is in India will get a news feed a very similar news feed can be given to another young software engineer who likes football in India so you can group users into a single cohort and give them similar news feeds now when one user from this cohort asks for a news feed you generate it from the database the first time store it in local memory and give the response the next time when a similar user comes instead of querying the database you just query your local memory right because you have this information stored instead of recomputing it instead of recalling the database you take your already stored result and give it back as a response and so the whole idea behind caching is reducing repeatable work through storage instead of doing the same competition again and again you store it in local memory give it back as a response and usually caches are much faster to query than a database because caches are closer to your system here as an example if you take the cache query time to be 1 millisecond and the response to be 1 millisecond if all of the queries can be answered through cash it'll be almost 10% in terms of savings and this idea can be extended even to the client even to the mobile device that you have when you fetch your news feed and you get a response if the user is scrolling their Newsfeed again and again they come back they keep their phone they come back again you can reuse the newsfeed that you have already fetched from the network so instead of making a 200 millisecond call store it in your local device in a mini cache inside the phone and now you'll see that the response time of the app has gone down from almost 200 milliseconds to 2 milliseconds okay this sounds like magic uh and of course it is there are drawbacks of caching and there are some limitations to caching that we'll talk about but at a high level caching reduces latency by just using more storage so natural question here is why don't we take the entire database that we have and put it in memory put it in Cache for small systems that makes a lot of sense if you have some static data which is in GBS it makes sense to just take all of that data and put it in the cache if it's being queried off of but for large databases fitting terabytes or petabytes of data into memory is just impossible right uh you may fit in a terabyte of data in memory but it'll be expensive what happens here is you start to optimize on the things that you store in Cache you have to take a part of the database a section of the database which is most frequently used Put It In memory so that when a user is quering for some information it's highly likely that the Cache can serve it it there's a distribution of how many people go to the cach and how many people go to the database so how many popular queries versus unpopular queries based on that you have the final computation let's say 90% people go to the cash 10% go to the database you'll see that in the end you will save less than 10% right 2 or 4 milliseconds so now your job as an engineer is to look at what data is going to be queried so you have to do some sort of prediction here and then store that in cash so that when the client actually call for the data it's already in cash and for that we have to ask ourselves two important questions one when there's an update to the cache how do we manage it because the cache is a copy of a database now when there is an update you will have to update the database and the cash together okay so do you do it together do you do it later there are many strategies here we look at all the right policies and the second part is what data do I evict what data do I kick out of cash if there is an overflow the reason for this is your cash memory is limited and your database is much much larger if a new video has become viral in your system where the video is in the database and all the users are quering for it you want to move this video into cash but your memory is full so what do you do you have to kick out something you have to evict some video already existing in the cache to bring in this new video this is a very common operation that the cach has to perform and so you need a particular policy and algorithm which tells you what video should I kick out okay what data do I evict that's called a cach policy that these two questions form a cach policy you might have heard of some of them least recently used least frequently used uh there's a lot of cash policies out there some of them are machine learning based also as a software engineer the most important ones to understand are least recently used and Le frequently used we'll talk about a few uh in this chapter later and as for right policies we talk about all of them in the upcoming lessons so we clearly know the benefits of a cache you know saved computation always sounds good low latency sounds good are there any drawbacks uh I always find drawbacks interesting because when it comes to such a fundamental component it's almost inevitable to have a drawback but we kind of ignore it and we try to mitigate it but it's good to know the drawbacks what if your cash is not storing the data that is being queried by users in that case all that will happen is that the request will come to the server the server is going to check the cache it's going to find that the data entry is missing it's going to go to the database and get the entry all that happened was this wasteful additional computation which is going to the cach and coming back so an unoptimized or a poor hit rate of a cache is actually going to hurt your system one scenario where this may occur is if you have your clients squaring your data in a sequence let's take 1 2 3 4 as a sequence and let's say your cache memory is limited to just three elements so the first request comes for one the cash has the entry missing it populates from the database gives back a response one is now in the cach then you have two then you have three and at four you have something interesting the cash is overloaded you query the data for from DB the DB gives you that entry and now you have to populate the cash what do you evict well the least recently used entry is one so you evict one you put in four and now now the client asks for one you go back to the cache you see that one doesn't exist you populate one from the database the entry which is eved is two the client asks for two then right so the client is going in a sequence and because of that because of your caching policy you're are having something called trashing where you're doing useless work you're doing a lot of evictions and loading into cash but none of that is helping your system it's not reducing latency it's increasing latency and it's wasteful memory usage the second problem is a much more well-known problem we'll try to mitigate it throughout the rest of the chapter in fact so uh the problem is eventual consistency if you have a copy of a data then the copy has to be updated along with the original source of Truth so in our case in most cases the database is the source of truth right the data that the database stores is the latest copy uh the cache has a stale copy or a working copy that you can do with I'll take an example here let's say you are seeing the number of likes on a YouTube video for every added like there's a query to the database but maybe the cach is updated every hour or maybe every minute right it doesn't need to have the latest number of likes in cash that helps reduce the work on the cash side but the drawback here is that the data is not true now if you have Financial transactions which are running caching systems then you may see older entries you may see stale entries which could cause problems okay so this is known as eventual consistency eventually the cash will be consistent but when well that is determined by the policy that you use as we'll see now the last part about caching is where do you place the cache do you place it in your servers so that will be an in-memory cache this can be a map which is running along with your application or you could place it in the database so the database is getting queries commonly used queries are going to be cached by the database itself in its server or you can have a global cache it's an external system it itself is a cache server which is being queried by API calls get and put by the rest of the services that you have in your system and this cache is independent it can scale independently it may be written in a different programming language uh if you change the algorithm of the cache there is no need to redeploy the code on your servers so there are certain benefits which one would you choose typically in a large scale production system all three are applied there's some inmemory data that you want to store you want to basically cache data across the system even on your client device you want to cash some data your database itself will have a small cache automatically as a blackbox we don't know about it but databases usually have a small cache for queries but as a software engineer what you are going to be concerned with most is having a distributed cache for a large scale distributed system the reason for this like we said is the cach can scale independently its deployments are independent and multiple Services can use the same cache all of that logic all of that algorithm without having to code it themselves so concluding this introduction to caching basically they save time but the caching policy matters and the placement of the cach matters depending on your system you need to make the right choices 